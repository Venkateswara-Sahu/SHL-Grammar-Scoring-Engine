# ğŸ™ï¸ Grammar Scoring Engine from Voice Samples

**SHL AI Research Intern Assessment - Option 2**

An advanced, production-ready system for evaluating grammar quality from voice recordings using state-of-the-art speech recognition and NLP techniques.

## ğŸŒŸ Features

- **High-Accuracy Transcription**: Whisper-based ASR with multiple model sizes
- **Multi-Dimensional Grammar Scoring**: 
  - Syntax correctness
  - Grammar error detection
  - Fluency metrics
  - Readability scores
- **Audio Quality Assessment**: Pre-transcription quality checks
- **Confidence Metrics**: Reliability indicators for each score
- **REST API**: Easy integration with FastAPI
- **Interactive Web Demo**: User-friendly interface
- **Comprehensive Evaluation**: Detailed metrics and visualizations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Audio     â”‚ -> â”‚   Audio      â”‚ -> â”‚     ASR     â”‚ -> â”‚   Grammar    â”‚
â”‚   Input     â”‚    â”‚ Preprocessingâ”‚    â”‚  (Whisper)  â”‚    â”‚   Analysis   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚                    â”‚                   â”‚
                          â†“                    â†“                   â†“
                   Quality Check         Confidence          Error Types
                                                                  â†“
                                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                          â”‚    Final     â”‚
                                                          â”‚    Score     â”‚
                                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### Prerequisites
- Python 3.9+
- FFmpeg (for audio processing)

### Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd SHL-Internship

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"
```

## ğŸš€ Quick Start

### Using the API

```bash
# Start the API server
python src/api/main.py

# In another terminal, test it
curl -X POST "http://localhost:8000/score" \
  -F "file=@path/to/audio.wav"
```

### Using Python

```python
from src.pipeline.grammar_scorer import GrammarScoringPipeline

# Initialize pipeline
pipeline = GrammarScoringPipeline()

# Score audio file
result = pipeline.score_audio("path/to/audio.wav")

print(f"Grammar Score: {result['grammar_score']:.2f}")
print(f"Transcription: {result['transcription']}")
print(f"Errors Found: {len(result['errors'])}")
```

## ğŸ“Š Output Format

```json
{
  "grammar_score": 8.5,
  "transcription": "This is the transcribed text.",
  "confidence": 0.92,
  "audio_quality": 0.87,
  "metrics": {
    "syntax_score": 9.0,
    "error_count": 2,
    "fluency_score": 8.8,
    "readability_score": 8.2
  },
  "errors": [
    {
      "type": "grammar",
      "message": "Subject-verb agreement error",
      "context": "They was going",
      "suggestion": "They were going"
    }
  ]
}
```

## ğŸ§ª Running Tests

```bash
pytest tests/ -v --cov=src
```

## ğŸ“ Project Structure

```
SHL-Internship/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessor.py      # Audio preprocessing
â”‚   â”‚   â””â”€â”€ quality_checker.py   # Audio quality assessment
â”‚   â”œâ”€â”€ asr/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ whisper_model.py     # Whisper ASR wrapper
â”‚   â”‚   â””â”€â”€ transcriber.py       # Transcription logic
â”‚   â”œâ”€â”€ grammar/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ analyzer.py          # Grammar analysis
â”‚   â”‚   â”œâ”€â”€ error_detector.py    # Error detection
â”‚   â”‚   â””â”€â”€ scorer.py            # Scoring algorithms
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ grammar_scorer.py    # Main pipeline
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”‚   â””â”€â”€ models.py            # Pydantic models
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py            # Configuration
â”‚       â””â”€â”€ metrics.py           # Evaluation metrics
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_evaluation.ipynb
â”‚   â””â”€â”€ 03_error_analysis.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_audio.py
â”‚   â”œâ”€â”€ test_grammar.py
â”‚   â””â”€â”€ test_pipeline.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Raw audio files
â”‚   â”œâ”€â”€ processed/               # Processed data
â”‚   â””â”€â”€ results/                 # Evaluation results
â”œâ”€â”€ models/
â”‚   â””â”€â”€ whisper/                 # Cached models
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env.example
```

## ğŸ¯ Scoring Methodology

### Grammar Score (0-10 scale)

The final score is a weighted combination of:

1. **Syntax Correctness** (30%): Parse tree analysis
2. **Grammar Errors** (30%): Error count and severity
3. **Fluency** (20%): Speech naturalness and flow
4. **Readability** (20%): Complexity and clarity

### Confidence Score

Based on:
- Audio quality (SNR, clarity)
- ASR confidence
- Grammar analysis certainty

## ğŸ”¬ Technical Approach

1. **Audio Preprocessing**
   - Noise reduction
   - Normalization
   - Silence removal
   - Quality assessment

2. **Speech Recognition**
   - Whisper (base/small/medium models)
   - Confidence scoring
   - Word-level timestamps

3. **Grammar Analysis**
   - LanguageTool for error detection
   - spaCy for syntax analysis
   - NLTK for additional metrics
   - Custom scoring algorithms

4. **Evaluation**
   - Precision, recall, F1 for error detection
   - Correlation with human ratings
   - Error type distribution

## ğŸ“ˆ Performance

- Average transcription time: ~2s per minute of audio
- Grammar analysis: ~0.5s per transcript
- API response time: <5s for typical audio samples

## ğŸ› ï¸ Technologies Used

- **Whisper**: State-of-the-art ASR
- **spaCy**: NLP and syntax analysis
- **LanguageTool**: Grammar checking
- **FastAPI**: High-performance API
- **PyTorch**: Deep learning framework
- **librosa**: Audio analysis

## ğŸ“ License

MIT

## ğŸ‘¤ Author

**Your Name**
- Email: your.email@example.com
- GitHub: @yourusername

## ğŸ™ Acknowledgments

- SHL AI Team for the opportunity
- OpenAI for Whisper
- The open-source community

---

*Built with â¤ï¸ for SHL AI Research Intern Assessment*
